{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306e0421",
   "metadata": {},
   "source": [
    "Initialise The PySpark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialise a PySpark session using the PySpark API\n",
    "import os\n",
    "from pyspark.sql import SparkSession # type: ignore\n",
    "from pyspark.sql.types import StringType,IntegerType,StructType,StructField # type: ignore\n",
    "from pyspark.sql.functions import * # type: ignore\n",
    "spark = SparkSession.builder.appName(\"MyPySparkAutomatic\").getOrCreate()\n",
    "\n",
    "# Your PySpark session is now created and you can use it to read data, perform transformations, and write data.\n",
    "print(\"SparkSession created successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3babb",
   "metadata": {},
   "source": [
    "Read The CSV Data(DataFrame: flight_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c014f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Spark DataFrame\n",
    "file_path = r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\\2015-summary.csv\"\n",
    "flight_df_1 = spark.read.format(\"csv\") \\\n",
    "\t.option(\"header\", \"true\") \\\n",
    "\t.option(\"inferSchema\", \"true\") \\\n",
    "\t.option(\"mode\", \"PERMISSIVE\") \\\n",
    "\t.load(file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "flight_df_1.show(n=flight_df_1.count(), truncate=False)\n",
    "\n",
    "#Show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", flight_df_1.count())\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "flight_df_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ca031",
   "metadata": {},
   "source": [
    "Create The Manual Schema(DataFrame: flight_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File paths\n",
    "file_path_0 = r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\"\n",
    "file_path_1 = os.path.join(file_path_0, \"2015-summary.csv\")\n",
    "file_path_2 = os.path.join(file_path_0, \"bad_records\")\n",
    "\n",
    "# Create a Manual schema for the DataFrame\n",
    "my_schema = StructType([StructField(\"COUNTRY_1\", StringType(), True),\n",
    "                        StructField(\"COUNTRY_2\", StringType(), True),\n",
    "                        StructField(\"TOTAL_COUNT\", IntegerType(), True),\n",
    "                        StructField(\"_corrupt_record\", StringType(), True)\n",
    "                        ])\n",
    "                        \n",
    "# Read the CSV file into a Spark DataFrame with the manual schema\n",
    "# Read CSV with schema and capture bad records\n",
    "flight_df_2 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(my_schema) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(file_path_1)\n",
    "\n",
    "#Shows the dataframe with bad records\n",
    "flight_df_2.show(n=flight_df_2.count(), truncate=False)\n",
    "\n",
    "# Filter and write bad records manually\n",
    "bad_df = flight_df_2.filter(\"`_corrupt_record` IS NOT NULL\")\n",
    "bad_df.write.mode(\"overwrite\").json(file_path_2)\n",
    "\n",
    "\n",
    "# List the contents of the directory\n",
    "import os\n",
    "print(\"Contents of the bad records directory:\")\n",
    "if os.path.exists(file_path_2):\n",
    "    for item in os.listdir(file_path_2):\n",
    "        print(item)\n",
    "else:\n",
    "    print(\"No bad records directory found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700aff0f",
   "metadata": {},
   "source": [
    "How to read json file in pyspark(DataFrame: people_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "file_path_0 = r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\"\n",
    "file_path_1 = os.path.join(file_path_0, \"2015-summary.csv\")\n",
    "file_path_2 = os.path.join(file_path_0, \"bad_records\")\n",
    "file_path_3 = os.path.join(file_path_0, \"multiline.json\")\n",
    "\n",
    "people_df=spark.read.format(\"json\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(file_path_3)\n",
    "# Show the DataFrame\n",
    "people_df.show(n=people_df.count(), truncate=False)\n",
    "# Show the total number of rows in the DataFrame\n",
    "people_df.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b7afd",
   "metadata": {},
   "source": [
    "How to read Parquet file in pyspark(DataFrame: people_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1038bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_0 = r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\"\n",
    "file_path_4 = os.path.join(file_path_0, \"part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet\")\n",
    "\n",
    "parquet_df = spark.read.format(\"parquet\") \\\n",
    "    .load(file_path_4)\n",
    "# Show the DataFrame\n",
    "parquet_df.show(n=parquet_df.count(), truncate=False)\n",
    "# Show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", parquet_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7eecdf",
   "metadata": {},
   "source": [
    "Finding Metadata Information In Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Path to your .parquet file\n",
    "file_path = r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet\"\n",
    "\n",
    "try:\n",
    "    # Read the whole Parquet file\n",
    "    df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "\n",
    "    # âœ… Display all rows and columns\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(df)\n",
    "\n",
    "    # ğŸ”½ Optionally, export to CSV for easier viewing\n",
    "    #output_csv = file_path.replace(\".parquet\", \".csv\")\n",
    "    #df.to_csv(output_csv, index=False)\n",
    "    #print(f\"\\nâœ… Full data exported to: {output_csv}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error reading Parquet file:\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "# Load the Parquet file\n",
    "parquet_file = pq.ParquetFile(\n",
    "    r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet\"\n",
    ")\n",
    "\n",
    "print(\"File metadata:\")\n",
    "print(parquet_file.metadata)\n",
    "\n",
    "print(\"\\nFirst row group metadata:\")\n",
    "print(parquet_file.metadata.row_group(0))\n",
    "\n",
    "print(\"\\nFirst column in first row group:\")\n",
    "print(parquet_file.metadata.row_group(0).column(0))\n",
    "\n",
    "print(\"\\nColumn statistics:\")\n",
    "print(parquet_file.metadata.row_group(0).column(0).statistics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9248a9",
   "metadata": {},
   "source": [
    "How To Write Dataframe on disk(dataframe:parquet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_0 = r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\"\n",
    "file_path_4 = os.path.join(file_path_0, \"part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet\")\n",
    "# Example: Write the parquet_df DataFrame to Parquet format\n",
    "parquet_df.repartition(2).write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(os.path.join(file_path_0, \"parquet_df_write_repartioned_2\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1679d",
   "metadata": {},
   "source": [
    "Implementing Partitioning & Bucketing in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file Using Partitioning\n",
    "parquet_df.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .partitionBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .save(os.path.join(file_path_0, \"parquet_df_write_partition_by_Example\"))\n",
    "\n",
    "# Write the DataFrame to a Parquet table Using Bucketing (CSV does not support bucketing)\n",
    "\"\"\"\n",
    "parquet_df.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .bucketBy(2, \"\") \\\n",
    "    .saveAsTable(\"parquet_df_write_bucket_by_Example\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b100f",
   "metadata": {},
   "source": [
    "Tranformation in PySpark:How To Create Dataframe API\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e29408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################DATA ENGINEERING PIPLINE######################################################################\n",
    "# READ----------------------------------------TRANSFORM----------------------------------------WRITE#\n",
    "                                #DataFrame API--------------------SPARK SQL#\n",
    "\n",
    "##Create a DataFrame using the DataFrame API for pe4rforming transformations in PySpark\n",
    "# Create data for dataframe\n",
    "data = [(1, 1),(2, 1),(3, 1),(4, 2),(5, 1),(6, 2),(7, 2)]\n",
    "\n",
    "# Create a schema DataFrame\n",
    "columns = [\"id\", \"num\"]\n",
    "# Create a DataFrame using the data and schema\n",
    "example_df = spark.createDataFrame(data, columns)\n",
    "# Show the DataFrame\n",
    "example_df.show()\n",
    "example_df.printSchema()\n",
    "print(example_df.columns) #it is an attriburt not callable function in pyspark\n",
    "example_df.count()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566e3cb",
   "metadata": {},
   "source": [
    "Transformation in PySpark:Using Select Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Shivam Gupta\\OneDrive\\Documents\\Shivam_Developement\\PYTHON\\python_tutorial\\employee_data.csv\"\n",
    "employee_df = spark.read.format(\"csv\") \\\n",
    "\t.option(\"header\", \"true\") \\\n",
    "\t.option(\"inferSchema\", \"true\") \\\n",
    "\t.option(\"mode\", \"PERMISSIVE\") \\\n",
    "\t.load(file_path)\n",
    "\n",
    "# Create a Transformation using the DataFrame API & storing in another DataFrame as variable\n",
    "employee_df_1 = employee_df.select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\"), employee_df.gender, employee_df[\"address\"])\n",
    "employee_df_2 = employee_df.select(expr(\"id+5\").alias(\"id_plus_5\"), expr(\"salary*2\").alias(\"salary_times_2\"), expr(\"concat(name, address)\").alias(\"name_address\"))\n",
    "\n",
    "#show the DataFrame\n",
    "employee_df_1.show(truncate=False)\n",
    "employee_df_2.show(truncate=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9bf8f",
   "metadata": {},
   "source": [
    "Transformation in PySpark Using Spark SQL:Query using the select statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crating a temporary view for the DataFrame\n",
    "employee_df.createOrReplaceTempView(\"employee_tbl\")\n",
    "# Create a SQL query to select the desired columns\n",
    "employee_tbl_1=spark.sql(\"\"\"select * from employee_tbl where salary > 70000\"\"\")\n",
    "employee_tbl_1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3d2e5",
   "metadata": {},
   "source": [
    "Transformation in Pyspark: Using Filter,Aliases,Literal,Casting,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ab0f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+------+-------+\n",
      "|id |salary|id_plus_5|gender|address|\n",
      "+---+------+---------+------+-------+\n",
      "|1  |75000 |6        |m     |INDIA  |\n",
      "|2  |100000|7        |f     |USA    |\n",
      "|3  |150000|8        |m     |INDIA  |\n",
      "|4  |200000|9        |m     |JAPAN  |\n",
      "|5  |300000|10       |m     |USA    |\n",
      "|6  |300000|11       |m     |INDIA  |\n",
      "|7  |540000|12       |m     |USA    |\n",
      "|8  |70000 |13       |m     |JAPAN  |\n",
      "|9  |150000|14       |m     |JAPAN  |\n",
      "|10 |25000 |15       |f     |RUSSIA |\n",
      "|11 |35000 |16       |f     |INDIA  |\n",
      "|12 |200000|17       |f     |INDIA  |\n",
      "|13 |650000|18       |m     |USA    |\n",
      "|14 |95000 |19       |m     |RUSSIA |\n",
      "|15 |750000|20       |m     |INDIA  |\n",
      "+---+------+---------+------+-------+\n",
      "\n",
      "+---+--------+---+------+-------+------+\n",
      "|id |name    |age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|4  |Prantosh|17 |200000|JAPAN  |m     |\n",
      "|8  |Praveen |28 |70000 |JAPAN  |m     |\n",
      "|9  |Dev     |32 |150000|JAPAN  |m     |\n",
      "+---+--------+---+------+-------+------+\n",
      "\n",
      "+---+------+---------+\n",
      "|id |salary|id_plus_5|\n",
      "+---+------+---------+\n",
      "|4  |200000|9        |\n",
      "|9  |150000|14       |\n",
      "+---+------+---------+\n",
      "\n",
      "+---+------+---------+\n",
      "|id |salary|id_plus_5|\n",
      "+---+------+---------+\n",
      "|4  |200000|9        |\n",
      "|9  |150000|14       |\n",
      "+---+------+---------+\n",
      "\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "|id |name    |age|salary|address|gender|last_name|\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "|1  |Manish  |26 |75000 |INDIA  |m     |Gupta    |\n",
      "|2  |Nikita  |23 |100000|USA    |f     |Gupta    |\n",
      "|3  |Pritam  |22 |150000|INDIA  |m     |Gupta    |\n",
      "|4  |Prantosh|17 |200000|JAPAN  |m     |Gupta    |\n",
      "|5  |Vikash  |31 |300000|USA    |m     |Gupta    |\n",
      "|6  |Rahul   |55 |300000|INDIA  |m     |Gupta    |\n",
      "|7  |Raju    |67 |540000|USA    |m     |Gupta    |\n",
      "|8  |Praveen |28 |70000 |JAPAN  |m     |Gupta    |\n",
      "|9  |Dev     |32 |150000|JAPAN  |m     |Gupta    |\n",
      "|10 |Sherin  |16 |25000 |RUSSIA |f     |Gupta    |\n",
      "|11 |Ragu    |12 |35000 |INDIA  |f     |Gupta    |\n",
      "|12 |Sweta   |43 |200000|INDIA  |f     |Gupta    |\n",
      "|13 |Raushan |48 |650000|USA    |m     |Gupta    |\n",
      "|14 |Mukesh  |36 |95000 |RUSSIA |m     |Gupta    |\n",
      "|15 |Prakash |52 |750000|INDIA  |m     |Gupta    |\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "|id |name    |age|salary|address|gender|last_name|\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "|1  |Manish  |26 |75000 |INDIA  |m     |Gupta    |\n",
      "|2  |Nikita  |23 |100000|USA    |f     |Gupta    |\n",
      "|3  |Pritam  |22 |150000|INDIA  |m     |Gupta    |\n",
      "|4  |Prantosh|17 |200000|JAPAN  |m     |Gupta    |\n",
      "|5  |Vikash  |31 |300000|USA    |m     |Gupta    |\n",
      "|6  |Rahul   |55 |300000|INDIA  |m     |Gupta    |\n",
      "|7  |Raju    |67 |540000|USA    |m     |Gupta    |\n",
      "|8  |Praveen |28 |70000 |JAPAN  |m     |Gupta    |\n",
      "|9  |Dev     |32 |150000|JAPAN  |m     |Gupta    |\n",
      "|10 |Sherin  |16 |25000 |RUSSIA |f     |Gupta    |\n",
      "|11 |Ragu    |12 |35000 |INDIA  |f     |Gupta    |\n",
      "|12 |Sweta   |43 |200000|INDIA  |f     |Gupta    |\n",
      "|13 |Raushan |48 |650000|USA    |m     |Gupta    |\n",
      "|14 |Mukesh  |36 |95000 |RUSSIA |m     |Gupta    |\n",
      "|15 |Prakash |52 |750000|INDIA  |m     |Gupta    |\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "\n",
      "+------+--------+-------+----------+-----------+----------+\n",
      "|emp_id|emp_name|emp_age|emp_salary|emp_address|emp_gender|\n",
      "+------+--------+-------+----------+-----------+----------+\n",
      "|1     |Manish  |26     |75000     |INDIA      |m         |\n",
      "|2     |Nikita  |23     |100000    |USA        |f         |\n",
      "|3     |Pritam  |22     |150000    |INDIA      |m         |\n",
      "|4     |Prantosh|17     |200000    |JAPAN      |m         |\n",
      "|5     |Vikash  |31     |300000    |USA        |m         |\n",
      "|6     |Rahul   |55     |300000    |INDIA      |m         |\n",
      "|7     |Raju    |67     |540000    |USA        |m         |\n",
      "|8     |Praveen |28     |70000     |JAPAN      |m         |\n",
      "|9     |Dev     |32     |150000    |JAPAN      |m         |\n",
      "|10    |Sherin  |16     |25000     |RUSSIA     |f         |\n",
      "|11    |Ragu    |12     |35000     |INDIA      |f         |\n",
      "|12    |Sweta   |43     |200000    |INDIA      |f         |\n",
      "|13    |Raushan |48     |650000    |USA        |m         |\n",
      "|14    |Mukesh  |36     |95000     |RUSSIA     |m         |\n",
      "|15    |Prakash |52     |750000    |INDIA      |m         |\n",
      "+------+--------+-------+----------+-----------+----------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|75000 |\n",
      "|100000|\n",
      "|150000|\n",
      "|200000|\n",
      "|300000|\n",
      "|300000|\n",
      "|540000|\n",
      "|70000 |\n",
      "|150000|\n",
      "|25000 |\n",
      "|35000 |\n",
      "|200000|\n",
      "|650000|\n",
      "|95000 |\n",
      "|750000|\n",
      "+------+\n",
      "\n",
      "+---+--------+---+------+-------+------+\n",
      "|id |name    |age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|1  |Manish  |26 |75000 |INDIA  |m     |\n",
      "|2  |Nikita  |23 |100000|USA    |f     |\n",
      "|3  |Pritam  |22 |150000|INDIA  |m     |\n",
      "|4  |Prantosh|17 |200000|JAPAN  |m     |\n",
      "|5  |Vikash  |31 |300000|USA    |m     |\n",
      "|6  |Rahul   |55 |300000|INDIA  |m     |\n",
      "|7  |Raju    |67 |540000|USA    |m     |\n",
      "|8  |Praveen |28 |70000 |JAPAN  |m     |\n",
      "|9  |Dev     |32 |150000|JAPAN  |m     |\n",
      "|10 |Sherin  |16 |25000 |RUSSIA |f     |\n",
      "|11 |Ragu    |12 |35000 |INDIA  |f     |\n",
      "|12 |Sweta   |43 |200000|INDIA  |f     |\n",
      "|13 |Raushan |48 |650000|USA    |m     |\n",
      "|14 |Mukesh  |36 |95000 |RUSSIA |m     |\n",
      "|15 |Prakash |52 |750000|INDIA  |m     |\n",
      "+---+--------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Alises the DataFrame\n",
    "employee_df_4 = employee_df.select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\"), employee_df.gender, employee_df[\"address\"])\n",
    "\n",
    "# Filtering the DataFrame using the DataFrame API\n",
    "employee_df_5 = employee_df.filter(col(\"address\") == \"JAPAN\")\n",
    "employee_df_6 = employee_df.filter((col(\"address\") == \"JAPAN\") & (col(\"salary\") > 70000)) \\\n",
    "    .select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\"))\n",
    "employee_df_7 = employee_df.select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\")).where(\"address = 'JAPAN' and salary > 70000\")\n",
    "\n",
    "#Literal function used to create a column with a constant value\n",
    "employee_df_8 = employee_df.select(\"*\", lit(\"Gupta\").alias(\"last_name\"))\n",
    "employee_df_9= employee_df.withColumn(\"last_name\", lit(\"Gupta\"))\n",
    "\n",
    "#Renaming the columns\n",
    "employee_df_10 = (\n",
    "    employee_df.withColumnRenamed(\"id\", \"emp_id\")\n",
    "    .withColumnRenamed(\"salary\", \"emp_salary\")\n",
    "    .withColumnRenamed(\"address\", \"emp_address\")\n",
    "    .withColumnRenamed(\"gender\", \"emp_gender\")\n",
    "    .withColumnRenamed(\"name\", \"emp_name\")\n",
    "    .withColumnRenamed(\"last_name\", \"emp_last_name\")\n",
    "    .withColumnRenamed(\"age\", \"emp_age\")\n",
    ")\n",
    "\n",
    "#Casting the column\n",
    "employee_df_11 = employee_df.withColumn(\"id\", col(\"id\").cast(StringType())).withColumn(\"salary\", col(\"salary\").cast(\"long\"))\n",
    "\n",
    "#Dropping the column\n",
    "employee_df_12 = employee_df.drop(\"last_name\", \"age\", \"address\", \"gender\", \"name\", \"id\",)\n",
    "\n",
    "# Show the DataFrame\n",
    "employee_df_4.show(truncate=False)\n",
    "employee_df_5.show(truncate=False)\n",
    "employee_df_6.show(truncate=False)\n",
    "employee_df_7.show(truncate=False)\n",
    "employee_df_8.show(truncate=False)\n",
    "employee_df_9.show(truncate=False)\n",
    "employee_df_10.show(truncate=False)\n",
    "employee_df_11.printSchema()\n",
    "employee_df_12.show(truncate=False)\n",
    "employee_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82bc39",
   "metadata": {},
   "source": [
    "Transformation in Pyspark: Union & Union All(Same in Datafreme API But Different in Spark SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Data for manager1\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "# Create a schema for the DataFrame\n",
    "schema=['id', 'name', 'sal', 'mngr_id']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "manager_df_1 = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame\n",
    "manager_df_1.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "manager_df_1.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_1.count())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create data for manager2\n",
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17)]\n",
    "# Create a schema for the DataFrame\n",
    "schema1=['id', 'name', 'sal', 'mngr_id']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "manager_df_2 = spark.createDataFrame(data1, schema1)\n",
    "# Show the DataFrame\n",
    "manager_df_2.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "manager_df_2.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_2.count())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Union of two DataFrames\n",
    "manager_df_union = manager_df_1.union(manager_df_2)\n",
    "manager_df_unionAll= manager_df_1.unionAll(manager_df_2)\n",
    "manager_df_unionByName= manager_df_1.unionByName(manager_df_2)\n",
    "\n",
    "# Show the DataFrame\n",
    "manager_df_union.show(truncate=False)\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_union.count())\n",
    "manager_df_unionAll.show(truncate=False)\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_unionAll.count())\n",
    "manager_df_unionByName.show(truncate=False)\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_unionByName.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4d6d2",
   "metadata": {},
   "source": [
    "Transformation in Pyspark: Case(if-else comaprison using when/otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cbe1062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n",
      "|id  |name   |age |salary|address|department |\n",
      "+----+-------+----+------+-------+-----------+\n",
      "|1   |manish |26  |20000 |india  |IT         |\n",
      "|2   |rahul  |NULL|40000 |germany|engineering|\n",
      "|3   |pawan  |12  |60000 |india  |sales      |\n",
      "|4   |roshini|44  |NULL  |uk     |engineering|\n",
      "|5   |raushan|35  |70000 |india  |sales      |\n",
      "|6   |NULL   |29  |200000|uk     |IT         |\n",
      "|7   |adam   |37  |65000 |us     |IT         |\n",
      "|8   |chris  |16  |40000 |us     |sales      |\n",
      "|NULL|NULL   |NULL|NULL  |NULL   |NULL       |\n",
      "|7   |adam   |37  |65000 |us     |IT         |\n",
      "+----+-------+----+------+-------+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "Total number of rows in the DataFrame: 10\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|id  |name   |age |salary|address|department |is_adult|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|1   |manish |26  |20000 |india  |IT         |Yes     |\n",
      "|2   |rahul  |NULL|40000 |germany|engineering|NULL    |\n",
      "|3   |pawan  |12  |60000 |india  |sales      |No      |\n",
      "|4   |roshini|44  |NULL  |uk     |engineering|Yes     |\n",
      "|5   |raushan|35  |70000 |india  |sales      |Yes     |\n",
      "|6   |NULL   |29  |200000|uk     |IT         |Yes     |\n",
      "|7   |adam   |37  |65000 |us     |IT         |Yes     |\n",
      "|8   |chris  |16  |40000 |us     |sales      |No      |\n",
      "|NULL|NULL   |NULL|NULL  |NULL   |NULL       |NULL    |\n",
      "|7   |adam   |37  |65000 |us     |IT         |Yes     |\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|id  |name   |age |salary|address|department |is_adult|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|1   |manish |26  |20000 |india  |IT         |medium  |\n",
      "|2   |rahul  |NULL|40000 |germany|engineering|major   |\n",
      "|3   |pawan  |12  |60000 |india  |sales      |minor   |\n",
      "|4   |roshini|44  |NULL  |uk     |engineering|major   |\n",
      "|5   |raushan|35  |70000 |india  |sales      |major   |\n",
      "|6   |NULL   |29  |200000|uk     |IT         |medium  |\n",
      "|7   |adam   |37  |65000 |us     |IT         |major   |\n",
      "|8   |chris  |16  |40000 |us     |sales      |minor   |\n",
      "|NULL|NULL   |NULL|NULL  |NULL   |NULL       |major   |\n",
      "|7   |adam   |37  |65000 |us     |IT         |major   |\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create data for DataFrame\n",
    "\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'age', 'salary', 'address', 'department']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "emp_df= spark.createDataFrame(emp_data, schema)\n",
    "# Show the DataFrame\n",
    "emp_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "emp_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", emp_df.count())\n",
    "\n",
    "\n",
    "\n",
    "#Checking the Age of the employee if they are adult or not(otherwise).Assuming emp_df is your original DataFrame\n",
    "emp_df_1 = emp_df.withColumn(\n",
    "    \"is_adult\",\n",
    "    when(col(\"age\").isNull(), None)           # If age is null â†’ null\n",
    "    .when(col(\"age\") > 18, \"Yes\")             # If age > 18 â†’ \"Yes\"\n",
    "    .otherwise(\"No\")                          # Otherwise â†’ \"No\"\n",
    ")\n",
    "\n",
    "\n",
    "emp_df_2 = emp_df.withColumn(\n",
    "    \"is_adult\",\n",
    "    when((col(\"age\")>0) &(col(\"age\")<18), \"minor\")          \n",
    "    .when((col(\"age\")>18) &(col(\"age\")<30), \"medium\")                        \n",
    "    .otherwise(\"major\")                          \n",
    ")\n",
    "\n",
    "# Show the DataFrame\n",
    "emp_df_1.show(truncate=False)\n",
    "emp_df_2.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e501f8a5",
   "metadata": {},
   "source": [
    "Transformation in Pyspark: Case(Unique & Sorted Record in datafarame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26e81c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "|id |name  |sal  |mngr_id|\n",
      "+---+------+-----+-------+\n",
      "|10 |Anil  |50000|18     |\n",
      "|11 |Vikas |75000|16     |\n",
      "|12 |Nisha |40000|18     |\n",
      "|13 |Nidhi |60000|17     |\n",
      "|14 |Priya |80000|18     |\n",
      "|15 |Mohit |45000|18     |\n",
      "|16 |Rajesh|90000|10     |\n",
      "|17 |Raman |55000|16     |\n",
      "|18 |Sam   |65000|17     |\n",
      "|15 |Mohit |45000|18     |\n",
      "|13 |Nidhi |60000|17     |\n",
      "|14 |Priya |90000|18     |\n",
      "|18 |Sam   |65000|17     |\n",
      "+---+------+-----+-------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sal: long (nullable = true)\n",
      " |-- mngr_id: long (nullable = true)\n",
      "\n",
      "Total number of rows in the DataFrame: 13\n",
      "+---+------+-----+-------+\n",
      "|id |name  |sal  |mngr_id|\n",
      "+---+------+-----+-------+\n",
      "|10 |Anil  |50000|18     |\n",
      "|11 |Vikas |75000|16     |\n",
      "|12 |Nisha |40000|18     |\n",
      "|13 |Nidhi |60000|17     |\n",
      "|14 |Priya |80000|18     |\n",
      "|15 |Mohit |45000|18     |\n",
      "|16 |Rajesh|90000|10     |\n",
      "|17 |Raman |55000|16     |\n",
      "|18 |Sam   |65000|17     |\n",
      "|14 |Priya |90000|18     |\n",
      "+---+------+-----+-------+\n",
      "\n",
      "+---+------+\n",
      "|id |name  |\n",
      "+---+------+\n",
      "|10 |Anil  |\n",
      "|11 |Vikas |\n",
      "|12 |Nisha |\n",
      "|13 |Nidhi |\n",
      "|14 |Priya |\n",
      "|15 |Mohit |\n",
      "|16 |Rajesh|\n",
      "|17 |Raman |\n",
      "|18 |Sam   |\n",
      "+---+------+\n",
      "\n",
      "+---+------+-----+-------+\n",
      "|id |name  |sal  |mngr_id|\n",
      "+---+------+-----+-------+\n",
      "|10 |Anil  |50000|18     |\n",
      "|11 |Vikas |75000|16     |\n",
      "|12 |Nisha |40000|18     |\n",
      "|13 |Nidhi |60000|17     |\n",
      "|14 |Priya |80000|18     |\n",
      "|15 |Mohit |45000|18     |\n",
      "|16 |Rajesh|90000|10     |\n",
      "|17 |Raman |55000|16     |\n",
      "|18 |Sam   |65000|17     |\n",
      "|14 |Priya |90000|18     |\n",
      "+---+------+-----+-------+\n",
      "\n",
      "+---+------+-----+-------+\n",
      "|id |name  |sal  |mngr_id|\n",
      "+---+------+-----+-------+\n",
      "|14 |Priya |90000|18     |\n",
      "|16 |Rajesh|90000|10     |\n",
      "|14 |Priya |80000|18     |\n",
      "|11 |Vikas |75000|16     |\n",
      "|18 |Sam   |65000|17     |\n",
      "|13 |Nidhi |60000|17     |\n",
      "|17 |Raman |55000|16     |\n",
      "|10 |Anil  |50000|18     |\n",
      "|15 |Mohit |45000|18     |\n",
      "|12 |Nisha |40000|18     |\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data for DataFrame\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(13 ,'Nidhi',60000,  17),      \n",
    "(14 ,'Priya',90000,  18),  \n",
    "(18 ,'Sam',65000,   17)\n",
    "]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'sal', 'mngr_id']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "mngr_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame\n",
    "mngr_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "mngr_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", mngr_df.count())\n",
    "\n",
    "\n",
    "\n",
    "# Finding unique records & deleting/droping duplicates in the DataFrame\n",
    "mngr_df_1 = mngr_df.distinct()\n",
    "mngr_df_2 = mngr_df.select(\"id\", \"name\").distinct() #selecting distinct records from dataframe created using the id & name columns\n",
    "mngr_df_3 = mngr_df.dropDuplicates([\"id\", \"name\", \"sal\", \"mngr_id\"]) #droping duplicates from the DataFrame using the id & name columns\n",
    "\n",
    "\n",
    "#sorting the DataFrame\n",
    "mngr_df_4 = mngr_df_1.sort(col(\"sal\").desc(),col(\"name\").asc()) #sorting the DataFrame using the sal column\n",
    "\n",
    "\n",
    "\n",
    "#show the schema of the DataFrame\n",
    "mngr_df_1.show(truncate=False)\n",
    "mngr_df_2.show(truncate=False)\n",
    "mngr_df_3.show(truncate=False)\n",
    "mngr_df_4.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025abe3",
   "metadata": {},
   "source": [
    "Transformation in Pyspark: Aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30eb0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n",
      "|id  |name   |age |salary|address|department |\n",
      "+----+-------+----+------+-------+-----------+\n",
      "|1   |manish |26  |20000 |india  |IT         |\n",
      "|2   |rahul  |NULL|40000 |germany|engineering|\n",
      "|3   |pawan  |12  |60000 |india  |sales      |\n",
      "|4   |roshini|44  |NULL  |uk     |engineering|\n",
      "|5   |raushan|35  |70000 |india  |sales      |\n",
      "|6   |NULL   |29  |200000|uk     |IT         |\n",
      "|7   |adam   |37  |65000 |us     |IT         |\n",
      "|8   |chris  |16  |40000 |us     |sales      |\n",
      "|NULL|NULL   |NULL|NULL  |NULL   |NULL       |\n",
      "|7   |adam   |37  |65000 |us     |IT         |\n",
      "+----+-------+----+------+-------+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "Total number of rows in the DataFrame: 10\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|10      |\n",
      "+--------+\n",
      "\n",
      "+-----------+\n",
      "|count(name)|\n",
      "+-----------+\n",
      "|8          |\n",
      "+-----------+\n",
      "\n",
      "+----------------------+\n",
      "|distinct_address_count|\n",
      "+----------------------+\n",
      "|4                     |\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create data for DataFrame\n",
    "empl_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'age', 'salary', 'address', 'department']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "empl_df= spark.createDataFrame(empl_data, schema)\n",
    "# Show the DataFrame\n",
    "empl_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "empl_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", empl_df.count())\n",
    "\n",
    "\n",
    "#count() function is used to count the number of rows in the DataFrame\n",
    "empl_df_1 = empl_df.select(count(\"*\"))\n",
    "empl_df_2 = empl_df.select(count(\"name\")) \n",
    "empl_df_3 = empl_df.select(countDistinct(\"address\").alias(\"distinct_address_count\")) #counting the distinct records in the DataFrame using the address column\n",
    "\n",
    "#min().max() and avg() function is used to find the minimum, maximum and average of the column in the DataFrame\n",
    "empl_df_4 = empl_df.select(min(\"salary\").alias(\"min_salary\"), max(\"salary\").alias(\"max_salary\"), avg(\"salary\").alias(\"avg_salary\")) #finding the min, max and avg of the salary column in the DataFrame\n",
    "\n",
    "#show the DataFrame\n",
    "empl_df_1.show(truncate=False)\n",
    "empl_df_2.show(truncate=False)\n",
    "empl_df_3.show(truncate=False)\n",
    "empl_df_4.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fbe463",
   "metadata": {},
   "source": [
    "Transformation in Pyspark: Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d14548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+\n",
      "|id |name   |salary|department|\n",
      "+---+-------+------+----------+\n",
      "|1  |manish |50000 |IT        |\n",
      "|2  |vikash |60000 |sales     |\n",
      "|3  |raushan|70000 |marketing |\n",
      "|4  |mukesh |80000 |IT        |\n",
      "|5  |pritam |90000 |sales     |\n",
      "|6  |nikita |45000 |marketing |\n",
      "|7  |ragini |55000 |marketing |\n",
      "|8  |rakesh |100000|IT        |\n",
      "|9  |aditya |65000 |IT        |\n",
      "|10 |rahul  |50000 |marketing |\n",
      "+---+-------+------+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "Total number of rows in the DataFrame: 10\n",
      "+----------+-----+----------+----------+----------+\n",
      "|department|count|avg_salary|min_salary|max_salary|\n",
      "+----------+-----+----------+----------+----------+\n",
      "|IT        |4    |73750.0   |50000     |100000    |\n",
      "|sales     |2    |75000.0   |60000     |90000     |\n",
      "|marketing |4    |55000.0   |45000     |70000     |\n",
      "+----------+-----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data for DataFrame\n",
    "data=[(1,'manish',50000,\"IT\"),\n",
    "(2,'vikash',60000,\"sales\"),\n",
    "(3,'raushan',70000,\"marketing\"),\n",
    "(4,'mukesh',80000,\"IT\"),\n",
    "(5,'pritam',90000,\"sales\"),\n",
    "(6,'nikita',45000,\"marketing\"),\n",
    "(7,'ragini',55000,\"marketing\"),\n",
    "(8,'rakesh',100000,\"IT\"),\n",
    "(9,'aditya',65000,\"IT\"),\n",
    "(10,'rahul',50000,\"marketing\")]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'salary', 'department']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "dept_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame\n",
    "dept_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "dept_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", dept_df.count())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#groupby() function is used to group the DataFrame by the department column\n",
    "dept_df_2 = dept_df.groupBy(\"department\").agg(count(\"*\").alias(\"count\"), avg(\"salary\").alias(\"avg_salary\"), min(\"salary\").alias(\"min_salary\"), max(\"salary\").alias(\"max_salary\")) #counting the number of records in the DataFrame using the department column and finding the min, max and avg of the salary column in the DataFrame\n",
    "\n",
    "# Show the DataFrame\n",
    "dept_df_2.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2286ca5",
   "metadata": {},
   "source": [
    "Transformation in Pyspark: Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c256a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
